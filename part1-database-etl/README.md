# Part 1 – Database Design and ETL Pipeline

## Objective

The objective of Part 1 is to design a normalized relational database and build an ETL (Extract, Transform, Load) pipeline that ingests raw CSV data into the database after performing necessary data cleaning and transformation.

---

## Description

FlexiMart provides raw CSV files containing customer, product, and sales data with multiple data quality issues such as missing values, duplicates, inconsistent formats, and invalid references.  

In this part, a Python-based ETL pipeline is implemented to:
- Read raw data from CSV files
- Clean and standardize the data
- Load the processed data into a MySQL relational database
- Enable business analytics using SQL queries

---

## Tasks Completed

- Designed relational database schema in Third Normal Form (3NF)
- Implemented ETL pipeline using Python and Pandas
- Handled data quality issues:
  - Duplicate records
  - Missing values
  - Inconsistent phone numbers and category names
  - Inconsistent date formats
- Loaded cleaned data into MySQL tables
- Generated a data quality report
- Implemented SQL business queries for analytics

---

## Files Included

- `etl_pipeline.py` – Python script for ETL processing
- `schema_documentation.md` – Entity-Relationship description and normalization explanation
- `business_queries.sql` – SQL queries for business reporting
- `data_quality_report.txt` – Summary of data processing generated by ETL
- `requirements.txt` – Required Python dependencies

---

## Output

- Clean and consistent transactional database (`fleximart`)
- Ready-to-use tables for analytical queries
- Documented database schema and relationships
